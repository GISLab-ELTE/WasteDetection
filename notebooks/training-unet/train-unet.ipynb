{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import traceback\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import skimage as sk\n",
    "import rasterio\n",
    "import gc\n",
    "import os\n",
    "from PIL import Image\n",
    "from numpy import random\n",
    "import cv2\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "import pydensecrf.densecrf as dcrf\n",
    "from pydensecrf.utils import unary_from_labels, create_pairwise_bilateral, create_pairwise_gaussian\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, cohen_kappa_score, jaccard_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_IMAGE_PATH = \"\"\n",
    "LOADED_IMAGE_SAVE_PATH = \"loaded_ims.npy\"\n",
    "LOADED_LABEL_SAVE_PATH = \"loaded_labels.npy\"\n",
    "MODEL_SAVE_PATH = \"\"\n",
    "MODEL_LOAD_PATH = \"\"\n",
    "TEST_IMAGE_SAVE_PATH = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preparation\n",
    "\n",
    "Loads .TIF images and their classified label from a specified directory given in `root_directory`. The image and the label must be in the same folder and the if the image's name is `image.tif`, then the label should have the name `image_classified.tif`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_tif_images_from_directory(root_directory):\n",
    "    tif_images = []\n",
    "    tif_labels = []\n",
    "\n",
    "    for root, dirs, files in os.walk(root_directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.tif') and not file.lower().endswith('classified.tif'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "\n",
    "                    ds = gdal.Open(file_path, gdal.GA_ReadOnly)\n",
    "\n",
    "                    rows = ds.RasterYSize\n",
    "                    cols = ds.RasterXSize\n",
    "                    bands = ds.RasterCount\n",
    "                    array = ds.ReadAsArray().astype(dtype=\"float32\")\n",
    "\n",
    "                    array = np.stack(array, axis=2)\n",
    "\n",
    "                    array = np.reshape(array, [rows, cols, bands])\n",
    "                    array = np.transpose(array, (2,0,1))\n",
    "                    max = array.max(axis=(1,2), keepdims=True)\n",
    "                    array = array/max                  \n",
    "                    label_path = file_path[:-4] + \"_classified.tif\"                   \n",
    "                    label_img = rasterio.open(label_path)\n",
    "                    label_img = label_img.read(1).astype('f4')\n",
    "                    label_img = np.asarray(label_img)\n",
    "                     \n",
    "                    tif_labels.append(label_img)\n",
    "                    tif_images.append(array)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    \"\"           \n",
    "    return tif_images, tif_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pads the input `array` to the specified `target_shape`. If the given `array` is a label image (it has only two dimensions e.g (128,128)), then it applies the padding to every dimension, if `array` is 3-dimensional, then the we don't have to apply the padding to the channel dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_shape(array, target_shape):\n",
    "    if len(array.shape) == 2:\n",
    "        padding = []\n",
    "        for current_size, target_size in zip(array.shape, target_shape):\n",
    "            pad_total = max(target_size - current_size, 0)\n",
    "            pad_before = pad_total // 2\n",
    "            pad_after = pad_total - pad_before\n",
    "            padding.append((pad_before, pad_after))\n",
    "        \n",
    "    elif len(array.shape) == 3: \n",
    "        padding = [(0, 0)]\n",
    "        for current_size, target_size in zip(array.shape[1:], target_shape):\n",
    "            pad_total = max(target_size - current_size, 0)\n",
    "            pad_before = pad_total // 2\n",
    "            pad_after = pad_total - pad_before\n",
    "            padding.append((pad_before, pad_after))\n",
    "    \n",
    "    else:\n",
    "        print(\"invalid input\")\n",
    "\n",
    "    padded_array = np.pad(array, padding, mode='constant', constant_values=0)\n",
    "    \n",
    "    return padded_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `sk.measure.regionprops` (https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_regionprops.html) we cut out a 128x128 shaped patch from the original image, centered around the center of the bounding box of a labeled object. If this patch would be smaller than the desired size, we use the previous `pad_to_shape` function. The number of patches which contain no waste can be specified using `no_waste_counter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims, labels = load_tif_images_from_directory(TRAINING_IMAGE_PATH)\n",
    "cut_ims = []\n",
    "cut_labels = []\n",
    "no_waste_counter = 0\n",
    "for i in range(len(labels)):\n",
    "    labeled_img = sk.measure.label(labels[i])\n",
    "    regions = sk.measure.regionprops(labeled_img)\n",
    "\n",
    "    for props in regions:\n",
    "        minr, minc, maxr, maxc = props.bbox\n",
    "        center = np.array(((minr + maxr)//2,(minc + maxc)//2))\n",
    "        start_x = center[0]-64\n",
    "        start_y = center[1]-64\n",
    "        end_x = (start_x + 128)\n",
    "        end_y = (start_y + 128)\n",
    "        if start_x < 0:\n",
    "            start_x = 0\n",
    "        if start_y < 0:\n",
    "            start_y = 0\n",
    "        if end_x > ims[i].shape[1]:\n",
    "            end_x = ims[i].shape[1]\n",
    "        if end_y > ims[i].shape[2]:\n",
    "            end_y = ims[i].shape[2]\n",
    "\n",
    "\n",
    "\n",
    "        cut = ims[i][:,start_x:end_x, start_y:end_y]\n",
    "        cut_label = labels[i][start_x:end_x, start_y:end_y]\n",
    "        if cut.shape != (4,128,128):\n",
    "            cut = pad_to_shape(cut, (128, 128))\n",
    "            cut_label = pad_to_shape(cut_label, (128,128))\n",
    "        cut_label_filtered = np.copy(cut_label)\n",
    "        cut_label_filtered[cut_label != 100] = 0\n",
    "        cut_label_filtered[cut_label == 100] = 1\n",
    "        if not np.isin(1, cut_label_filtered) and no_waste_counter < 850:\n",
    "                cut_ims.append(cut)\n",
    "                cut_labels.append(cut_label_filtered)\n",
    "                no_waste_counter += 1\n",
    "        elif np.isin(1, cut_label_filtered):\n",
    "            cut_ims.append(cut)\n",
    "            cut_labels.append(cut_label_filtered)\n",
    "\n",
    "\n",
    "cut_ims = np.stack(cut_ims,axis=0)\n",
    "cut_labels = np.stack(cut_labels, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly permutate the images and labels so that the images with no waste, and the images with waste are more evenly distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(cut_ims.shape[0])\n",
    "cut_ims = cut_ims[perm,:,:,:]\n",
    "cut_labels = cut_labels[perm,:,:]\n",
    "np.save(LOADED_IMAGE_SAVE_PATH,cut_ims)\n",
    "np.save(LOADED_LABEL_SAVE_PATH, cut_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_ims = np.load(LOADED_IMAGE_SAVE_PATH)\n",
    "cut_labels = np.load(LOADED_LABEL_SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `WasteTransforms` class transforms the images and the labels, using the same randomly generated seed to get the same results. Some color distortion can be applied to the images, but not the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteTransforms:\n",
    "    def __init__(self):\n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.RandomAffine(degrees=12, translate=(0.2, 0.2), scale=(0.85, 1.15)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            # transforms.ColorJitter(brightness=0.1, hue=0.1, contrast=0.1, saturation=0.1)\n",
    "        ])\n",
    "\n",
    "        self.label_transform = transforms.Compose([\n",
    "            transforms.RandomAffine(degrees=12, translate=(0.2, 0.2), scale=(0.85, 1.15)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "        ])\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        seed = torch.randint(0, 2**32, (1,)).item()\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        transformed_image = self.image_transform(image)\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        transformed_label = self.label_transform(label)\n",
    "        \n",
    "        return transformed_image, transformed_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `WasteDataSet` class must define the functions `__init__`, `__getitem__`, and `__lenn__` from the `torch.utils.data.Dataset` class. We convert the `numpy.ndarrays` to `torch.tensors` so that the neural networks can work with them. In `__getitem__`, we apply the transformations to the images and labels (only on the training dataset). During training, we iterate through the image using `torch.utils.data.DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WasteDataSet(Dataset):\n",
    "    def __init__(self, ims, labels ,transforms):\n",
    "        self.ims = torch.tensor(ims.astype(np.float32), dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ims)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.ims[idx]\n",
    "        label = self.labels[idx]\n",
    "        label = label.unsqueeze(0)\n",
    "        if self.transforms is not None:\n",
    "            image, label = self.transforms(image, label)\n",
    "        label = label.squeeze()\n",
    "        return image, label\n",
    "    \n",
    "split1 = round(len(cut_ims)*0.6)\n",
    "split2 = round(len(cut_ims)*.99)\n",
    "\n",
    "train_transforms = WasteTransforms()\n",
    "\n",
    "train_set = WasteDataSet(cut_ims[:split1], cut_labels[:split1], transforms= train_transforms)\n",
    "val_set = WasteDataSet(cut_ims[split1:split2], cut_labels[split1:split2], transforms=None)\n",
    "test_set = WasteDataSet(cut_ims[split2:], cut_labels[split2:],  transforms=None)\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=24, shuffle=True)\n",
    "val_dataloader = DataLoader(val_set, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size= 8, shuffle=True, drop_last=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used to align the output of different layers, due to the `//2` sometimes they are 1 pixel smaller/larger than another layer's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_layers(layers):\n",
    "    \n",
    "    layer_heights = []\n",
    "    layer_widths = []\n",
    "    rtn = []\n",
    "    for layer in layers:\n",
    "        layer_heights.append(layer.size(2))\n",
    "        layer_widths.append(layer.size(3))\n",
    "    target_height = min(layer_heights)\n",
    "    target_width = min(layer_widths)\n",
    "    for layer, layer_height, layer_width in zip(layers, layer_heights, layer_widths):\n",
    "        diff_y = (layer_height - target_height) // 2\n",
    "        diff_x = (layer_width - target_width) // 2\n",
    "        rtn.append(layer[:, :, diff_y:(diff_y + target_height), diff_x:(diff_x + target_width)])\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return torch.cat(rtn, dim = 1)\n",
    "\n",
    "def center_crop(layer, target_height, target_width):\n",
    "    _, _, layer_height, layer_width = layer.size()\n",
    "    diff_y = (layer_height - target_height) // 2\n",
    "    diff_x = (layer_width - target_width) // 2\n",
    "    return layer[:, :, diff_y:(diff_y + target_height), diff_x:(diff_x + target_width)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The building blocks of the `UNET` and `UNETPP` networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding = 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "        \n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = ConvolutionBlock(in_channels, out_channels)\n",
    "        self.mpool = nn.MaxPool2d((2,2))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        skip = self.conv(x)\n",
    "        out = self.mpool(skip)\n",
    "        return skip, out\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = ConvolutionBlock(out_channels + out_channels, out_channels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x, skip):\n",
    "        out = self.up_conv(x)\n",
    "        if out.size(2) != skip.size(2) or out.size(3) != skip.size(3):\n",
    "            skip = center_crop(skip, out.size(2), out.size(3))\n",
    "        \n",
    "        out = torch.cat([out, skip], axis=1)\n",
    "        out = self.conv(out)\n",
    "        return out\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels): \n",
    "        super().__init__()               \n",
    "        self.up_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride= 2),\n",
    "            nn.Dropout(0.2),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.up_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `UNET` class defines the architecture that is described in the paper https://arxiv.org/pdf/1505.04597. Number of in and out channels can be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels = 4,out_channels = 1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.e1 = EncoderBlock(in_channels, 64)\n",
    "        self.e2 = EncoderBlock(64, 128)\n",
    "        self.e3 = EncoderBlock(128,256)\n",
    "        self.e4 = EncoderBlock(256, 512)\n",
    "        \n",
    "        self.b = ConvolutionBlock(512, 1024)\n",
    "        \n",
    "        self.d1 = DecoderBlock(1024, 512)\n",
    "        self.d2 = DecoderBlock(512,256)\n",
    "        self.d3 = DecoderBlock(256, 128)\n",
    "        self.d4 = DecoderBlock(128,64)\n",
    "        self.output = nn.Conv2d(64, out_channels, kernel_size=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip1, out = self.e1(x)\n",
    "        skip2, out = self.e2(out)\n",
    "        skip3, out = self.e3(out)\n",
    "        skip4, out = self.e4(out)\n",
    "        \n",
    "        out = self.b(out)\n",
    "        out = self.d1(out, skip4)\n",
    "        out = self.d2(out, skip3)\n",
    "        out = self.d3(out, skip2)\n",
    "        out = self.d4(out, skip1)\n",
    "        out = torch.squeeze(out)\n",
    "        out = self.output(out)\n",
    "        \n",
    "        return out\n",
    "    def predict(self, x):\n",
    "        out = self.forward(x)\n",
    "        if self.out_channels == 1:\n",
    "            out = torch.sigmoid(out)  \n",
    "        elif self.out_channels > 1:\n",
    "            out = torch.softmax(out) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `UNETPP` class defines the architecture that is described in the paper https://arxiv.org/pdf/1912.05074. It can be given a pretrained `UNET` as a backbone, with an option to freeze the pretrained weights. Deep supervision can also be enabled/disabled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNETPP(nn.Module):\n",
    "    def __init__(self,pretrained_unet = None, freeze_weights = False ,deep_vision=False, in_channels = 4, out_channels = 1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.pretrained_unet = pretrained_unet\n",
    "        self.deep_vision = deep_vision\n",
    "        self.upsamp = nn.Upsample(scale_factor=2,mode='bilinear' , align_corners=True)\n",
    "        if self.pretrained_unet is not None:    \n",
    "            self.conv0_0 = self.pretrained_unet.e1\n",
    "            self.conv1_0 = self.pretrained_unet.e2\n",
    "            self.conv2_0 = self.pretrained_unet.e3\n",
    "            self.conv3_0 = self.pretrained_unet.e4\n",
    "            self.conv4_0 = EncoderBlock(512, 1024)\n",
    "            if freeze_weights:\n",
    "                for param in self.conv0_0.parameters():\n",
    "                    param.requires_grad = False\n",
    "                for param in self.conv1_0.parameters():\n",
    "                    param.requires_grad = False\n",
    "                for param in self.conv2_0.parameters():\n",
    "                    param.requires_grad = False\n",
    "                for param in self.conv3_0.parameters():\n",
    "                    param.requires_grad = False\n",
    "        else:\n",
    "            self.conv0_0 = EncoderBlock(in_channels, 64)\n",
    "            self.conv1_0 = EncoderBlock(64, 128)\n",
    "            self.conv2_0 = EncoderBlock(128, 256)\n",
    "            self.conv3_0 = EncoderBlock(256, 512)\n",
    "            self.conv4_0 = EncoderBlock(512, 1024)\n",
    "\n",
    "        self.conv0_1 = ConvolutionBlock(64 + 128, 64)\n",
    "        self.conv0_2 = ConvolutionBlock(2*64 + 128, 64)\n",
    "        self.conv0_3 = ConvolutionBlock(3*64 + 128, 64)\n",
    "        self.conv0_4 = ConvolutionBlock(4*64 + 128, 64)\n",
    "        \n",
    "        self.conv1_1 = ConvolutionBlock(128 + 256, 128)\n",
    "        self.conv1_2 = ConvolutionBlock(2*128 + 256, 128)\n",
    "        self.conv1_3 = ConvolutionBlock(3*128 + 256, 128)\n",
    "        \n",
    "        self.conv2_1 = ConvolutionBlock(256 + 512, 256)\n",
    "        self.conv2_2 = ConvolutionBlock(2*256 + 512, 256)\n",
    "        \n",
    "        self.conv3_1 = ConvolutionBlock(512 + 1024, 512)\n",
    "\n",
    "        self.up_conv1_0 = UpConv(128, 128)\n",
    "        self.up_conv2_0 = UpConv(256, 256)\n",
    "        self.up_conv3_0 = UpConv(512, 512)\n",
    "        \n",
    "        self.up_conv1_1 = UpConv(128, 128)\n",
    "        self.up_conv2_1 = UpConv(256, 256)\n",
    "        self.up_conv1_2 = UpConv(128, 128)\n",
    "\n",
    "        self.up_conv4_0 = UpConv(1024, 1024)\n",
    "        self.up_conv3_1 = UpConv(512, 512)\n",
    "        self.up_conv2_2 = UpConv(256, 256)\n",
    "        self.up_conv1_3 = UpConv(128, 128)\n",
    "        \n",
    "        if self.deep_vision:\n",
    "            self.deep1 = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "            self.deep2 = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "            self.deep3 = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "            self.deep4 = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.deep = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skip0_0, x0_0 = self.conv0_0(x)\n",
    "        skip1_0, x1_0 = self.conv1_0(x0_0)\n",
    "        skip2_0, x2_0 = self.conv2_0(x1_0)\n",
    "        skip3_0, x3_0 = self.conv3_0(x2_0)\n",
    "        skip4_0, x4_0 = self.conv4_0(x3_0)\n",
    "        \n",
    "    \n",
    "        x0_1 = self.conv0_1(torch.cat([skip0_0, self.up_conv1_0(skip1_0)], dim=1))\n",
    "        x1_1 = self.conv1_1(torch.cat([skip1_0, self.up_conv2_0(skip2_0)], dim=1))\n",
    "        x0_2 = self.conv0_2(torch.cat([skip0_0, x0_1, self.up_conv1_1(x1_1)], dim=1))\n",
    "        \n",
    "        x2_1 = self.conv2_1(torch.cat([skip2_0, self.up_conv3_0(skip3_0)], dim=1))\n",
    "        x1_2 = self.conv1_2(torch.cat([skip1_0, x1_1, self.up_conv2_1(x2_1)], dim=1))\n",
    "        x0_3 = self.conv0_3(torch.cat([skip0_0, x0_1, x0_2, self.up_conv1_2(x1_2)], dim=1))\n",
    "        \n",
    "        x3_1 = self.conv3_1(torch.cat([skip3_0, self.up_conv4_0(skip4_0)], dim=1))\n",
    "        x2_2 = self.conv2_2(torch.cat([skip2_0, x2_1, self.up_conv3_1(x3_1)], dim=1))\n",
    "        x1_3 = self.conv1_3(torch.cat([skip1_0, x1_1, x1_2, self.up_conv2_2(x2_2)], dim=1))\n",
    "        x0_4 = self.conv0_4(torch.cat([skip0_0, x0_1, x0_2, x0_3, self.up_conv1_3(x1_3)], dim=1))\n",
    "        \n",
    "             \n",
    "        \n",
    "        if self.deep_vision:\n",
    "            out1 = self.deep1(x0_1)\n",
    "            out2 = self.deep2(x0_2)\n",
    "            out3 = self.deep3(x0_3)\n",
    "            out4 = self.deep4(x0_4)\n",
    "            return [out1, out2, out3, out4]\n",
    "        else:\n",
    "            out = self.deep(x0_4)    \n",
    "        return out\n",
    "    \n",
    "    def predict(self, x):\n",
    "        skip0_0, x0_0 = self.conv0_0(x)\n",
    "        skip1_0, x1_0 = self.conv1_0(x0_0)\n",
    "        skip2_0, x2_0 = self.conv2_0(x1_0)\n",
    "        skip3_0, x3_0 = self.conv3_0(x2_0)\n",
    "        skip4_0, x4_0 = self.conv4_0(x3_0)\n",
    "        \n",
    "        x0_1 = self.conv0_1(align_layers([skip0_0, self.up_conv1_0(skip1_0)]))\n",
    "        x1_1 = self.conv1_1(align_layers([skip1_0, self.up_conv2_0(skip2_0)]))\n",
    "        x0_2 = self.conv0_2(align_layers([skip0_0, x0_1, self.up_conv1_1(x1_1)]))\n",
    "        x2_1 = self.conv2_1(align_layers([skip2_0, self.up_conv3_0(skip3_0)]))\n",
    "        x1_2 = self.conv1_2(align_layers([skip1_0, x1_1, self.up_conv2_1(x2_1)]))\n",
    "        x0_3 = self.conv0_3(align_layers([skip0_0, x0_1, x0_2, self.up_conv1_2(x1_2)]))\n",
    "        x3_1 = self.conv3_1(align_layers([skip3_0, self.up_conv4_0(skip4_0)]))\n",
    "        x2_2 = self.conv2_2(align_layers([skip2_0, x2_1, self.up_conv3_1(x3_1)]))\n",
    "        x1_3 = self.conv1_3(align_layers([skip1_0, x1_1, x1_2, self.up_conv2_2(x2_2)]))\n",
    "        x0_4 = self.conv0_4(align_layers([skip0_0, x0_1, x0_2, x0_3, self.up_conv1_3(x1_3)]))\n",
    "        \n",
    "             \n",
    "        \n",
    "        if self.deep_vision:\n",
    "            out1 = self.deep1(x0_1)\n",
    "            out2 = self.deep2(x0_2)\n",
    "            out3 = self.deep3(x0_3)\n",
    "            out4 = self.deep4(x0_4)\n",
    "            x = out4.size(2)\n",
    "            y = out4.size(3)\n",
    "            out1 = center_crop(out1, x, y)\n",
    "            out2 = center_crop(out2, x, y)\n",
    "            out3 = center_crop(out3, x, y)\n",
    "            out = out1 + out2 + out3 + out4\n",
    "            out = out / 4\n",
    "        else:\n",
    "            out = self.deep(x0_4)\n",
    "            \n",
    "        if self.out_channels == 1:\n",
    "            out = torch.sigmoid(out)  \n",
    "        elif self.out_channels > 1:\n",
    "            out = torch.softmax(out)\n",
    "        return out  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes for `R2UNET` and `R2AttentionUNET` (https://arxiv.org/pdf/1802.06955, https://arxiv.org/pdf/1804.03999), might not be the best suited for this task, but interesting nonetheless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentBlock(nn.Module):\n",
    "    def __init__(self, out_channel, t = 2):\n",
    "        super().__init__()\n",
    "        self.t = t\n",
    "        self.out_channel = out_channel\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(self.out_channel, self.out_channel, kernel_size=3, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(self.t):\n",
    "            if i == 0:\n",
    "                x1 = self.conv_layer(x)\n",
    "                \n",
    "            x1 = self.conv_layer(x+x1)\n",
    "        return x1 \n",
    "class RecurrentResidualBlock(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels, t = 2, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.rec_res_layer = nn.Sequential(\n",
    "            RecurrentBlock(out_channel=out_channels, t = t),\n",
    "            RecurrentBlock(out_channel=out_channels, t = t),\n",
    "            nn.Dropout2d(dropout)\n",
    "        )\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        out = self.rec_res_layer(x)\n",
    "        return x + out\n",
    "    \n",
    "class UpSampleConv(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up_sample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.up_sample(x)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, padding=0, bias= True),\n",
    "            nn.BatchNorm2d(F_int),\n",
    "            nn.Dropout2d(0.1)\n",
    "        )\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int),\n",
    "            nn.Dropout2d(0.1)\n",
    "            \n",
    "        )\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid(),\n",
    "            \n",
    "        )\n",
    "        self.relu = nn.Sequential(\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Dropout2d(0.5)  \n",
    "            )\n",
    "        \n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        x1 = center_crop(x1, g1.size(2), g1.size(3))\n",
    "        psi = self.psi(g1 + x1)\n",
    "        psi = self.relu(psi)\n",
    "        x = center_crop(x, psi.size(2), psi.size(3))\n",
    "        return x*psi\n",
    "    \n",
    "class R2UNET(nn.Module):\n",
    "    def __init__(self, in_channels = 4, out_channels = 1, t = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rec_res_layer1 = RecurrentResidualBlock(in_channels, 64, t=t)\n",
    "        self.rec_res_layer2 = RecurrentResidualBlock(64, 128, t=t)\n",
    "        self.rec_res_layer3 = RecurrentResidualBlock(128, 256, t=t)\n",
    "        self.rec_res_layer4 = RecurrentResidualBlock(256, 512, t=t, dropout=0.3)\n",
    "        self.rec_res_layer5 = RecurrentResidualBlock(512, 1024, t=t, dropout=0.3)\n",
    "        \n",
    "        self.up1 = UpSampleConv(1024, 512)\n",
    "        self.up2 = UpSampleConv(512, 256)\n",
    "        self.up3 = UpSampleConv(256, 128)\n",
    "        self.up4 = UpSampleConv(128, 64)\n",
    "        \n",
    "        self.up_rec_res_layer1 = RecurrentResidualBlock(1024, 512, t=t, dropout=0.3)\n",
    "        self.up_rec_res_layer2 = RecurrentResidualBlock(512, 256, t=t, dropout=0.3)\n",
    "        self.up_rec_res_layer3 = RecurrentResidualBlock(256, 128, t=t)\n",
    "        self.up_rec_res_layer4 = RecurrentResidualBlock(128, 64, t=t)\n",
    "        \n",
    "        self.out = nn.Conv2d(64, out_channels=out_channels, kernel_size=1) \n",
    "        \n",
    "        self.max = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        x1 = self.rec_res_layer1(x)\n",
    "        x2 = self.max(x1)\n",
    "        x2 = self.rec_res_layer2(x2)\n",
    "        x3 = self.max(x2)\n",
    "        x3 = self.rec_res_layer3(x3)\n",
    "        x4 = self.max(x3)\n",
    "        x4 = self.rec_res_layer4(x4)\n",
    "        x5 =self.max(x4)\n",
    "        x5 = self.rec_res_layer5(x5)\n",
    "        \n",
    "        d1 = self.up1(x5)\n",
    "        d1 = torch.cat((x4, d1), dim = 1)\n",
    "        d1 = self.up_rec_res_layer1(d1)\n",
    "        \n",
    "        d2 = self.up2(d1)\n",
    "        d2 = torch.cat((x3, d2), dim = 1)\n",
    "        d2 = self.up_rec_res_layer2(d2)\n",
    "        \n",
    "        d3 = self.up3(d2)\n",
    "        d3 = torch.cat((x2, d3), dim = 1)\n",
    "        d3 = self.up_rec_res_layer3(d3)\n",
    "        \n",
    "        d4 = self.up4(d3)\n",
    "        d4 = torch.cat((x1, d4), dim = 1)\n",
    "        d4 = self.up_rec_res_layer4(d4)\n",
    "        \n",
    "        out = self.out(d4)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2AttentionUNET(nn.Module):\n",
    "    def __init__(self, in_channels= 4, out_channels = 1, t = 2):\n",
    "        super().__init__()\n",
    "        self.max = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.upsamp = nn.Upsample(scale_factor=2)\n",
    "        \n",
    "        self.rec_res_layer1 = RecurrentResidualBlock(in_channels,64,t)\n",
    "        self.rec_res_layer2 = RecurrentResidualBlock(64, 128, t)\n",
    "        self.rec_res_layer3 = RecurrentResidualBlock(128, 256, t)\n",
    "        self.rec_res_layer4 = RecurrentResidualBlock(256, 512, t, dropout=0.3)\n",
    "        self.rec_res_layer5 = RecurrentResidualBlock(512, 1024, t, dropout=0.3)\n",
    "    \n",
    "        self.up1 = UpSampleConv(1024, 512)\n",
    "        self.up2 = UpSampleConv(512, 256)\n",
    "        self.up3 = UpSampleConv(256, 128)\n",
    "        self.up4 = UpSampleConv(128, 64)\n",
    "\n",
    "        self.att1 = AttentionBlock(512, 512, 256)\n",
    "        self.att2 = AttentionBlock(256, 256, 128)\n",
    "        self.att3 = AttentionBlock(128, 128, 64)\n",
    "        self.att4 = AttentionBlock(64,64,32)\n",
    "        \n",
    "        self.up_rec_res_layer1 = RecurrentResidualBlock(1024, 512, t, dropout=0.3)\n",
    "        self.up_rec_res_layer2 = RecurrentResidualBlock(512, 256, t)\n",
    "        self.up_rec_res_layer3 = RecurrentResidualBlock(256, 128, t)\n",
    "        self.up_rec_res_layer4 = RecurrentResidualBlock(128,64, t)\n",
    "        \n",
    "        self.out = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x1 = self.rec_res_layer1(x)\n",
    "\n",
    "        x2 = self.max(x1)\n",
    "        x2 = self.rec_res_layer2(x2)\n",
    "\n",
    "        x3 = self.max(x2)\n",
    "        x3 =  self.rec_res_layer3(x3)\n",
    "\n",
    "        x4 = self.max(x3)\n",
    "        x4 = self.rec_res_layer4(x4)\n",
    "        \n",
    "        x5 = self.max(x4)\n",
    "        x5 = self.rec_res_layer5(x5)\n",
    "\n",
    "        d1 = self.up1(x5)\n",
    "        x4 = self.att1(g=d1, x=x4)\n",
    "        d1 = torch.cat((x4,d1), dim=1)\n",
    "        d1 = self.up_rec_res_layer1(d1)\n",
    "    \n",
    "        d2 = self.up2(d1)\n",
    "        x3 = self.att2(g=d2, x=x3)\n",
    "        d2 = torch.cat((x3,d2), dim=1)\n",
    "        d2 = self.up_rec_res_layer2(d2)\n",
    "        \n",
    "        d3 = self.up3(d2)\n",
    "        x2 = self.att3(g=d3, x=x2)\n",
    "        d3 = torch.cat((x2,d3), dim=1)\n",
    "        d3 = self.up_rec_res_layer3(d3)\n",
    "        \n",
    "        d4 = self.up4(d3)\n",
    "        x1 = self.att4(g=d4, x=x1)\n",
    "        d4 = torch.cat((x1,d4), dim=1)\n",
    "        d4 = self.up_rec_res_layer4(d4)\n",
    "        \n",
    "        out = self.out(d4)\n",
    "        return out\n",
    "    def center_crop(self, layers):\n",
    "        \n",
    "        layer_heights = []\n",
    "        layer_widths = []\n",
    "        rtn = []\n",
    "        for layer in layers:\n",
    "            layer_heights.append(layer.size(2))\n",
    "            layer_widths.append(layer.size(3))\n",
    "        target_height = min(layer_heights)\n",
    "        target_width = min(layer_widths)\n",
    "        for layer, layer_height, layer_width in zip(layers, layer_heights, layer_widths):\n",
    "            diff_y = (layer_height - target_height) // 2\n",
    "            diff_x = (layer_width - target_width) // 2\n",
    "            rtn.append(layer[:, :, diff_y:(diff_y + target_height), diff_x:(diff_x + target_width)])\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        return torch.cat(rtn, dim = 1)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        x1 = self.rec_res_layer1(x)\n",
    "\n",
    "        x2 = self.max(x1)\n",
    "        x2 = self.rec_res_layer2(x2)\n",
    "\n",
    "        x3 = self.max(x2)\n",
    "        x3 =  self.rec_res_layer3(x3)\n",
    "\n",
    "        x4 = self.max(x3)\n",
    "        x4 = self.rec_res_layer4(x4)\n",
    "\n",
    "        x5 = self.max(x4)\n",
    "        x5 = self.rec_res_layer5(x5)\n",
    "\n",
    "        d1 = self.up1(x5)\n",
    "        x4 = self.att1(g=d1, x=x4)\n",
    "        d1 = self.center_crop((x4,d1))\n",
    "        d1 = self.up_rec_res_layer1(d1)\n",
    "    \n",
    "        d2 = self.up2(d1)\n",
    "        x3 = self.att2(g=d2, x=x3)\n",
    "        d2 = self.center_crop((x3,d2))\n",
    "        d2 = self.up_rec_res_layer2(d2)\n",
    "        \n",
    "        d3 = self.up3(d2)\n",
    "        x2 = self.att3(g=d3, x=x2)\n",
    "        d3 = self.center_crop((x2,d3))\n",
    "        d3 = self.up_rec_res_layer3(d3)\n",
    "        \n",
    "        d4 = self.up4(d3)\n",
    "        x1 = self.att4(g=d4, x=x1)\n",
    "        d4 = self.center_crop((x1,d4))\n",
    "        d4 = self.up_rec_res_layer4(d4)\n",
    "        out = self.out(d4)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some loss functions designed to handle the class imbalance that is present in the task. The function `torch.nn.BCEWithLogitsLoss` with `pos_weight` can also be used for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    def forward(self, predicition, target):\n",
    "        p = torch.sigmoid(predicition)\n",
    "        ce_loss = F.binary_cross_entropy_with_logits(predicition, target, reduction=\"none\")\n",
    "        p_t = p * target + (1 - p) * (1 - target)\n",
    "        loss = ce_loss * ((1 - p_t) ** self.gamma)\n",
    "        alpha_t = self.alpha * target + (1 - self.alpha) * (1 - target)\n",
    "        loss = alpha_t * loss \n",
    "        return loss.mean()\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self, gamma, pos_weight):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.pos_weight = pos_weight\n",
    "        self.epsilon = 1e-8\n",
    "        \n",
    "    def forward(self, inputs, labels):\n",
    "        inputs = torch.flatten(inputs)\n",
    "\n",
    "        labels = torch.flatten(labels)\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(input=inputs, target=labels, pos_weight=self.pos_weight)\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "        intersect = (inputs*labels).sum()\n",
    "        dice_loss = 1-(2.*intersect + self.epsilon) / (inputs.sum() + labels.sum() + self.epsilon)\n",
    "        \n",
    "        return self.gamma*dice_loss + (1-self.gamma)*bce_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train` function trains the neural network. It uses the DataLoaders defined earlier to iterate through the images. It needs an `optimizer` and `scheduler`, a `loss_function` that is suited for the task, the number of epochs (`n_epochs`) for training. `delta` can be used together with `patience` for early stopping, and to only save the models which have shown improvement. If the batch size would be to small due to GPU memory limitations, `accumulation_steps` can make it so that we only step with the optimizer after `accumulation_Steps` number of batches have been processed, practically increasing the batch size. `deep_vision` and `pretrained` are for UNET++ models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device,train_dataloader, val_dataloader, optimizer, scheduler,loss_function ,n_epochs=50, delta=0.01, patience=10, accumulation_steps = 1, deep_vision = 0, pretrained = 0):\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_loss = np.inf\n",
    "    no_improvement_count = 0\n",
    "    print(\"training began\")\n",
    "    try:\n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            train_epoch_losses = []\n",
    "            for i, (x, y) in enumerate(train_dataloader):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                optimizer.zero_grad() if i % accumulation_steps == 0 else None\n",
    "                if not deep_vision:\n",
    "                    y_hat = model(x)\n",
    "                    y_hat = y_hat.squeeze()\n",
    "                    y = y.squeeze()\n",
    "                    loss = loss_function(y_hat, y)\n",
    "                else:\n",
    "                    y_hat = model(x)\n",
    "                    loss = 0\n",
    "                    for y_h in y_hat:\n",
    "                        y = y.squeeze()\n",
    "                        y_h = y_h.squeeze()\n",
    "                        loss += loss_function(y_h,y)\n",
    "                    loss /= 4\n",
    "\n",
    "                train_epoch_losses.append(loss)\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                if (i + 1) % accumulation_steps == 0:\n",
    "                    optimizer.step() \n",
    "                    optimizer.zero_grad()  \n",
    "\n",
    "            train_epoch_losses = torch.tensor(train_epoch_losses)\n",
    "            avg_epoch_loss = train_epoch_losses.mean()\n",
    "            train_losses.append(avg_epoch_loss)\n",
    "\n",
    "            model.eval()\n",
    "            val_epoch_losses = []\n",
    "            for x, y in val_dataloader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                with torch.no_grad():\n",
    "                    y_hat = model(x)\n",
    "                if not deep_vision:\n",
    "                    y_hat = y_hat.squeeze()\n",
    "                    y = y.squeeze()\n",
    "                    loss = loss_function(y_hat, y)\n",
    "                else:    \n",
    "                    loss = 0\n",
    "                    for y_h in y_hat:\n",
    "                        y = y.squeeze()\n",
    "                        y_h = y_h.squeeze()\n",
    "                        loss += loss_function(y_h,y)\n",
    "                    loss /= 4\n",
    "\n",
    "                loss = loss.mean()\n",
    "                val_epoch_losses.append(loss)\n",
    "\n",
    "            val_epoch_losses = torch.tensor(val_epoch_losses)\n",
    "            avg_epoch_loss = val_epoch_losses.mean()\n",
    "            val_losses.append(avg_epoch_loss)\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "            print(f'Tranining {epoch+1}/{n_epochs} done, training loss: {train_losses[-1]}, validation loss: {val_losses[-1]}')\n",
    "\n",
    "\n",
    "            if avg_epoch_loss + delta < best_loss:\n",
    "                best_loss = avg_epoch_loss\n",
    "\n",
    "                path = MODEL_SAVE_PATH + (epoch+1).__str__() + '.sav'\n",
    "\n",
    "                torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'deep_vision': deep_vision,\n",
    "            'pretrained': pretrained\n",
    "\n",
    "                }, path)\n",
    "                no_improvement_count = 0\n",
    "\n",
    "            if avg_epoch_loss < delta:\n",
    "                delta /= 10\n",
    "            if patience < no_improvement_count:\n",
    "                print(\"early stopping\")\n",
    "                return train_losses, val_losses\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "\n",
    "        return train_losses, val_losses\n",
    "    except KeyboardInterrupt as e:\n",
    "        print(\"Training interrupted by user\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(\"Error occured during training:\")\n",
    "        traceback.print_exc()\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the learning rate, optimizer, and scheduler, and loss_function for the training process, and start the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gc.collect())\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "u_model = UNET()\n",
    "\n",
    "lr = 0.0005\n",
    "n_epoch = 50\n",
    "patience = 10\n",
    "delta = 0.01\n",
    "accumulation_steps = 1\n",
    "is_pretrained = 0\n",
    "is_deep_supervision = 0\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, u_model.parameters()), lr, weight_decay=1e-5)\n",
    "scheduler = OneCycleLR(optimizer,max_lr=0.001, steps_per_epoch=len(train_dataloader), epochs=50)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "weight = torch.tensor([15.0]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=weight)\n",
    "\n",
    "train_losses, val_losses  =  train(model=u_model, device = device, train_dataloader = train_dataloader,val_dataloader= val_dataloader,\n",
    "                                   optimizer= optimizer, scheduler=scheduler, loss_function=loss_fn,\n",
    "                                   n_epochs=n_epoch, delta=delta, patience=patience, \n",
    "                                   accumulation_steps=accumulation_steps, pretrained=is_pretrained,deep_vision=is_deep_supervision)\n",
    "if train_losses is not None:\n",
    "    plt.plot(train_losses, label='Training')\n",
    "    plt.plot(val_losses, label='Validation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('CrossEntropy')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluates the model on the test set. It saves a figure which shows the original image, the continuous prediction, a binary classification where the treshold is 0.5 probability, and the ground truth. it also calculates some metrics for numerical evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(MODEL_LOAD_PATH)\n",
    "u_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "u_model.eval()\n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "focus = []\n",
    "support = []\n",
    "kappa = []\n",
    "jaccard = []\n",
    "accuracy = []\n",
    "\n",
    "\n",
    "for x, y in test_dataloader:\n",
    "    with torch.no_grad():   \n",
    "        j += 1\n",
    "        pred = u_model.predict(x)\n",
    "        curr_pred = pred\n",
    "        curr_pred = curr_pred.squeeze()  \n",
    "        print(pred.shape)  \n",
    "        for i in range(curr_pred.shape[0]):         \n",
    "            display = curr_pred[i].data.cpu().numpy()\n",
    "            display2 = np.copy(display)\n",
    "            display2[display > 0.5] = 1\n",
    "            display2[display <= 0.5] = 0\n",
    "            input_image = x[i].data.cpu().numpy()\n",
    "            input_image = np.transpose(input_image, (1,2,0))\n",
    "            input_image = np.ascontiguousarray(input_image)\n",
    "            f, ax = plt.subplots(2,2)\n",
    "            ax[0,0].imshow(input_image[:,:,:3])\n",
    "            ax[1,1].imshow(display)\n",
    "            ax[1,0].imshow(display2)\n",
    "            ax[0,1].imshow(y[i][:,:].data.cpu().numpy())\n",
    "            f.savefig(TEST_IMAGE_SAVE_PATH  +j.__str__() + \"_\" + i.__str__())\n",
    "            plt.close(f)\n",
    "            treshold = 0.5\n",
    "            cpu_y = y[i].data.cpu().numpy()\n",
    "            prediction = np.zeros(cpu_y.shape)\n",
    "            prediction[display > treshold] = 1\n",
    "            prediction = prediction.astype(int)\n",
    "            cpu_y = cpu_y.astype(int)\n",
    "            prediction = prediction.flatten()\n",
    "            cpu_y = cpu_y.flatten()\n",
    "            p,r,fs,s = precision_recall_fscore_support(cpu_y, prediction, zero_division=0)\n",
    "            epoch_jaccard = jaccard_score(cpu_y, prediction, zero_division=0)\n",
    "            epoch_kappa = cohen_kappa_score(cpu_y, prediction)\n",
    "            epoch_accuracy = accuracy_score(cpu_y, prediction)\n",
    "            if not np.isnan(epoch_kappa.item()):\n",
    "                kappa.append(epoch_kappa)\n",
    "            precision.append(p.mean())\n",
    "            recall.append(r.mean())\n",
    "            focus.append(fs.mean())\n",
    "            support.append(s)\n",
    "            jaccard.append(epoch_jaccard)\n",
    "            accuracy.append(epoch_accuracy)  \n",
    "precision = np.asarray(precision)\n",
    "recall = np.asarray(recall)\n",
    "focus = np.asarray(focus)\n",
    "\n",
    "kappa = np.asarray(kappa)\n",
    "jaccard = np.asarray(jaccard)\n",
    "accuracy = np.asarray(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writes the calculated metrics to a txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = open(\"metrics.txt\", \"a\")\n",
    "f.write(\"precision: \" + np.mean(precision).__str__())\n",
    "f.write(\"\\n\")\n",
    "f.write(\"recall: \" + np.mean(recall).__str__())        \n",
    "f.write(\"\\n\")    \n",
    "f.write(\"focus: \" + np.mean(focus).__str__())\n",
    "f.write(\"\\n\")\n",
    "f.write(\"kappa: \" + np.mean(kappa).__str__())\n",
    "f.write(\"\\n\")\n",
    "f.write(\"jaccard: \" +  np.mean(jaccard).__str__())\n",
    "f.write(\"\\n\")\n",
    "f.write(\"accuracy: \" + np.mean(accuracy).__str__())\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads the images from Drina, Raho, and Kiskore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drina_images(root_directory):\n",
    "    tif_images = []\n",
    "    paths = []\n",
    "    for root, dirs, files in os.walk(root_directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.tif') and not file.lower().endswith('classified.tif') and 'udm2' not in file.lower():\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    img = rasterio.open(file_path)\n",
    "                    \n",
    "                    red = img.read(1)\n",
    "                    green = img.read(2)\n",
    "                    blue = img.read(3)\n",
    "                    NIR = img.read(4)\n",
    "\n",
    "                    red_max = np.percentile(red, 99.9)\n",
    "                    green_max = np.percentile(green, 99.9)\n",
    "                    blue_max = np.percentile(blue, 99.9)\n",
    "                    nir_max = np.percentile(NIR, 99.9)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    rgb = [red.astype('f4')/red_max, green.astype('f4')/green_max, blue.astype('f4')/blue_max, NIR.astype('f4')/nir_max]\n",
    "                    rgb = np.asarray(rgb)\n",
    "\n",
    "\n",
    "                    tif_images.append(rgb)\n",
    "                    paths.append(file_path[:-4])\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    \"\"\n",
    "                    \n",
    "    return tif_images, paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a list of model names, their paths, and a save directory for the processed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = []\n",
    "model_names = []\n",
    "save_dir = []\n",
    "\n",
    "MODEL_FOLDER = \"\"\n",
    "SERVER_IMAGE_PATH = \"\"\n",
    "for root, dirs, files in os.walk(MODEL_FOLDER):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.sav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                save_dir.append(root + \"\\\\test_output\\\\\")\n",
    "                model_names.append(file[:-4])\n",
    "                model_paths.append(file_path)\n",
    "print(save_dir)\n",
    "print(model_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluates the models on the images of Drina, Raho, and Kiskore, and saves a `.tif` image next to the original image with the model name, and also saves a figure containing the original image, the continuous prediction, and the classification image in the `save_dir` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, paths = drina_images(SERVER_IMAGE_PATH)\n",
    "import tifffile\n",
    "\n",
    "for j in range(len(model_names)):\n",
    "    u_model = UNETPP().cuda()\n",
    "    checkpoint = torch.load(model_paths[j])\n",
    "    u_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    u_model.eval()\n",
    "    print(len(paths))\n",
    "    for i in range(len(images)):\n",
    "        img = np.asarray(images[i])\n",
    "        img = torch.tensor(img, dtype=torch.float32, device=torch.device(\"cuda:0\"))\n",
    "        img = img.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            pred = u_model.predict(img)\n",
    "\n",
    "        display = pred.data.cpu().numpy()\n",
    "        display = display.squeeze()\n",
    "        treshold = 0.9\n",
    "        display2 = np.copy(display)\n",
    "        display2[display > treshold] = 1\n",
    "        display2[display <= treshold] = 0\n",
    "        img_cpu = img.data.cpu().numpy()\n",
    "        img_cpu = np.squeeze(img_cpu)\n",
    "        img_cpu = np.transpose(img_cpu, (1,2,0))\n",
    "        f, ax = plt.subplots(1,3)\n",
    "\n",
    "        ax[0].imshow(img_cpu[:,:,:3])\n",
    "        ax[1].imshow(display)\n",
    "        ax[2].imshow(display2)\n",
    "        tifffile.imwrite(paths[i] + \"_\" + model_names[j]  + \"_\" + \"_classified.tif\", display)\n",
    "        f.savefig(save_dir[j] +  model_names[j] + \"_\" +i.__str__() + \".png\")\n",
    "        plt.close(f)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
